{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ea396a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "032b467b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 0.0/62.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 62.6/62.6 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "     ---------------------------------------- 0.0/121.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 121.1/121.1 kB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jon\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "     ---------------------------------------- 0.0/100.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 100.3/100.3 kB ? eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "     ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 163.8/163.8 kB ? eta 0:00:00\n",
      "Installing collected packages: urllib3, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 requests-2.31.0 urllib3-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\Jon\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e728468",
   "metadata": {},
   "outputs": [],
   "source": [
    "teamDict = {\n",
    "    \"Cardinals\": 1,\n",
    "    \"Falcons\": 2,\n",
    "    \"Ravens\": 3,\n",
    "    \"Bills\": 4,\n",
    "    \"Panthers\": 5,\n",
    "    \"Bears\": 6,\n",
    "    \"Bengals\": 7,\n",
    "    \"Browns\": 8,\n",
    "    \"Cowboys\": 9,\n",
    "    \"Broncos\": 10,\n",
    "    \"Lions\": 11,\n",
    "    \"Packers\": 12,\n",
    "    \"Texans\": 13,\n",
    "    \"Colts\": 14,\n",
    "    \"Jaguars\": 15,\n",
    "    \"Chiefs\": 16,\n",
    "    \"Rams\": 17,\n",
    "    \"Chargers\": 18,\n",
    "    \"Dolphins\": 19,\n",
    "    \"Vikings\": 20,\n",
    "    \"Patriots\": 21,\n",
    "    \"Saints\": 22,\n",
    "    \"Giants\": 23,\n",
    "    \"Jets\": 24,\n",
    "    \"Raiders\": 25,\n",
    "    \"Eagles\": 26,\n",
    "    \"Steelers\": 27,\n",
    "    \"49ers\": 28,\n",
    "    \"Niners\": 28,\n",
    "    \"Seahawks\": 29,\n",
    "    \"Buccaneers\": 30,\n",
    "    \"Titans\": 31,\n",
    "    \"Commanders\": 32,\n",
    "    \"Football Team\": 33,\n",
    "    \"Redskins\": 34,\n",
    "    \"Oilers\": 35,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac3b90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "def scrapeTeamPassing(year):    \n",
    "    url = f\"https://www.nfl.com/stats/team-stats/offense/passing/{year}/reg/all\"\n",
    "    # Get web data from nfl.com\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find(\"table\")\n",
    "    \n",
    "    data = []\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        row_data = []\n",
    "        for cell in row.find_all([\"th\", \"td\"]):\n",
    "            row_data.append(cell.get_text(strip=True))\n",
    "        row_data[0] = row_data[0][:len(row_data[0])//2]\n",
    "        team_id = teamDict.get(row_data[0])\n",
    "        row_data.insert(0, team_id)\n",
    "        row_data.append(year)\n",
    "        if  'T' in row_data[14]:\n",
    "            row_data[14] = int(row_data[14].replace('T',''))\n",
    "        data.append(row_data)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "\n",
    "    df = df.drop(df.columns[1], axis=1)\n",
    "    df = df.drop(df.index[0])\n",
    "    # df = df.drop(df.index[1])    \n",
    "    df[0] = df[0].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# df = scrapeTeamPassing(2023)\n",
    "\n",
    "# # Connect to the PostgreSQL database\n",
    "# engine = create_engine('postgresql://postgres:nfl123@localhost/nfl-data')\n",
    "\n",
    "# columns = ['team_id', 'att', 'cmp', 'cmp_pct', 'yards_att', 'yards', 'td', 'int', 'rate', 'firsts', 'first_pct', 'twenty_plus', 'forty_plus', 'long', 'sack', 'sack_yards', 'year']\n",
    "# df.columns = columns\n",
    "\n",
    "# df['id'] = df.apply(lambda row: str(row['team_id']) + str(row['year']), axis=1)\n",
    "\n",
    "# df.to_sql('team_passing', engine, if_exists='append', index=False)\n",
    "# print(f\"Team Passing data sent to database for {year}\") \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55527b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_and_send_to_db(year):\n",
    "    \n",
    "    df = scrapeTeamPassing(year)\n",
    "    \n",
    "    # Connect to the PostgreSQL database\n",
    "    engine = create_engine('postgresql://postgres:nfl123@localhost/nfl-data')\n",
    "    \n",
    "    columns = ['team_id', 'att', 'cmp', 'cmp_pct', 'yards_att', 'yards', 'td', 'int', 'rate', 'firsts', 'first_pct', 'twenty_plus', 'forty_plus', 'long', 'sack', 'sack_yards', 'year']\n",
    "    df.columns = columns\n",
    "    \n",
    "    df['id'] = df.apply(lambda row: str(row['team_id']) + str(row['year']), axis=1)\n",
    "    \n",
    "    df.to_sql('team_passing', engine, if_exists='append', index=False)\n",
    "    print(f\"Team Passing data sent to database for {year}\")    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
